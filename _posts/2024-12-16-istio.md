---
layout: post
title: Istio
date: 2024-12-16 13:10 -0800
categories: [network]
tags: [network, istio]
---

Istio used to have a micro-service architecture. However, starting from v1.5,
It moved back to a monolithic architecture. This largely simplifies the
management of Istio. After v1.5, we only need to worry about two components:
the proxy agent and istiod daemon. See more details in this
[blog](https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/).

It sounds simple with only two components, right? The reality is not. See below
pods I grabbed from an old installation.

```
$ k get pods -A | grep istio
istio-operator               istio-operator-6f7d5894cc-mw5bp          1/1     Running
istio-system                 istio-egressgateway-58c576f5f-w9wht      1/1     Running
istio-system                 istio-ingressgateway-775747ccd7-66pdg    1/1     Running
istio-system                 istiod-888fc4cbb-4trqq                   1/1     Running
```

First, let's talk about `istio-operator`. This pod actually should not exit. It
is discouraged to install istio using operator mode any more. It is deprecated
in istion 1.23 and will be removed
[in version 1.24](https://istio.io/latest/blog/2024/in-cluster-operator-deprecation-announcement/).
Quote from the
[source code](https://github.com/istio/istio/blob/765457711ef83305458d91d7bca5f185fb6bb68b/operator/README.md?plain=1#L15)

> The operator formerly acted as an in-cluster operator, dynamically
> reconciling and Istio installation. This mode has now been removed, and it
> only serves as a client-side CLI tool to install Istio.

We can safely remove this pod/deployment if we use helm or istioctl to install
istio.

Second, let's turn to (ingress/egress)gateways. First, we should not mistake
them with k8s gateway api. They are related, but not the same.

A quick recap on k8s gateway api. Gateway is similar to ingress. GatewayClass
to Gateway is what IngressClass to Ingress. However, gateway is more powerful.
Gateway supports L4 and L7 routing. While ingress only supports http routing.
Ingress can only route traffic to a single namespace, it canâ€™t be used as a
unified gateway across multiple namespaces. Gateway api becomes available since
k8s 1.24. However, it is not natively implemented inside k8s. It exists as an
add-on. You need to manually install
[those CRDs](https://gateway-api.sigs.k8s.io/guides/#installing-gateway-api) or
choose an [implementation](https://gateway-api.sigs.k8s.io/implementations/)
which handles it for you.

Finally, John Howard has an
[excellent blog](https://blog.howardjohn.info/posts/istio-install/) talking
about all these istio confusions. Please check it out!

## Pilot

Pilot has two components: `pilot-agent` and `pilot-discovery`. If you have read
my Envoy notes below, you know that the xDS protocol has two parts: proxy and
management server. `pilot-agent` is the proxy part, and `pilot-discovery` is
the management server part.

### Pilot-agent

It runs in the istio-proxy container. Ex below:

```
/usr/local/bin/pilot-agent proxy sidecar --domain filebeat.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2

/usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --drain-strategy immediate --parent-shutdown-time-s 60 --service-cluster airflow.data --service-node sidecar~172.31.76.240~airflow-worker-0.data~data.svc.cluster.local --local-address-ip-version v4 --bootstrap-version 3 --disable-hot-restart --log-format %Y-%m-%dT%T.%fZ.%l.envoy %n.%v -l warning --component-log-level misc:error --concurrency 2
```

It is mainly responsible for managing envoy proxy sidecar.

### Pilot-discovery

It runs in `istiod` pod. Ex below:

```
/usr/local/bin/pilot-discovery discovery --monitoringAddr=:15014 --log_output_level=default:info --domain cluster.local --keepaliveMaxServerConnectionAge 30m
```

Pilot-discovery is responsible for reacting to k8s events, converting these
events to xDS responses and sending them to pilot-agents. We use two examples
to illustrate how this part works.

**Example 1: When a new service is deployed in a k8s cluster**

As you can imagine, pilot-discovery subscribes service create/update/delete
events in a k8s cluster. When an event happens, pilot-discovery will update its
internal service registry and push this change to all envoy proxies.

How does this process work? The answer is Kubernetes controllers. The
controllers for all types of resources inside Kubernetes such as namespace,
node, service, pod and etc are defined in
[this file](https://github.com/istio/istio/blob/3ea5695508654a77c4c1638134c0a8aca7e9996b/pilot/pkg/serviceregistry/kube/controller/controller.go#L203).
Especially for services, it register a handler for service event:

```
	c.registerHandlers(c.serviceInformer, "Services", c.onServiceEvent, nil)
```

Inside the handler, it calls `xdsUpdater` to push the change to remote Envoy
proxies.

**Example 2: What happens when setting service mesh across two k8s clusters**

Istio is able to set up a service mesh across multiple k8s clusters. How is
this possible? To set it up,
[One step](https://istio.io/latest/docs/setup/install/multicluster/multi-primary/#enable-endpoint-discovery)
of the installation process is to run below command:

```
istioctl x create-remote-secret --context="${CTX_CLUSTER1}" --name=cluster1 | kubectl apply -f - --context="${CTX_CLUSTER2}"
```

It generates a yaml file like following

```
apiVersion: v1
kind: Secret
metadata:
  annotations:
    networking.istio.io/cluster: cluster1
  creationTimestamp: null
  labels:
    istio/multiCluster: "true"
  name: istio-remote-secret-cluster1
  namespace: istio-system
stringData:
  cluster1: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: <...>
        server: https://369A1EB0F8100069D5C96281F5BD706D.gr7.us-east-2.eks.amazonaws.com
      name: cluster1
    contexts:
    - context:
        cluster: cluster1
        user: cluster1
      name: cluster1
    current-context: cluster1
    kind: Config
    preferences: {}
    users:
    - name: cluster1
      user:
        token: <...>
```

and then applies it to cluster2. Basically, cluster2 now has credentials to
operate on cluster1. See
[this code](https://github.com/istio/istio/blob/44de8ccfe4d25bb658fb8f1efec9da314db4bd8b/istioctl/pkg/multicluster/remote_secret.go#L86)
for more details about how this command works. The secret creation event will
be captured by `secretcontroller` because it listens all events on secrets with
label
[istio/multiCluster](https://github.com/istio/istio/blob/3fcff36ddc5e69ed20755c6385d44eb1a0e50505/pkg/kube/multicluster/secretcontroller.go#L50),
which registers a set of `ClusterHandler`s to handle secret
create/update/delete events.

#### How to use pilot-discovery debug endpoints

Pilot-discovery provides a set of
[debug endpoints](https://github.com/istio/istio/blob/cf3b4fdcba365825a1754ce2dca7a32544d6dfbd/pilot/pkg/xds/debug.go#L147-L194).
Here, we show some example use cases.

**Get all connected istioy-proxy instances**

This endpoint will return all pods that have istio-proxy injected.

```
$ ke istiod-64fb77c48d-x8cjj -- curl localhost:15014/debug/connections
{
    "totalClients": 21,
    "clients": [
      {
        "connectionId": "sidecar~172.31.90.176~airflow-scheduler-7bc4cf9fb8-bnb4r.data~data.svc.cluster.local-65372",
        "connectedAt": "2022-12-30T07:56:30.843188241Z",
        "address": "172.31.90.176:45502",
        "watches": null
      },
...
```

**Get Envoy config dump**

Envoy admin API has an endpoint
[GET /config_dump](https://www.envoyproxy.io/docs/envoy/latest/operations/admin#get--config_dump).
Istio has a debug endpoint to forward request to this `config_dump` endpoint.

```
ke istiod-64fb77c48d-x8cjj -- curl localhost:15014/debug/config_dump?proxyID=sidecar~172.31.90.176~airflow-scheduler-7bc4cf9fb8-bnb4r.data~data.svc.cluster.local-6537
```

The output is huge. Note the `proxyID` query parameter is required and it is
the `connnectionID` above.

**Get all services**

```
$ ke istiod-64fb77c48d-x8cjj -- curl localhost:15014/debug/registryz
[
{
  "Attributes": {
    "ServiceRegistry": "Kubernetes",
    "Name": "airflow-datadog-cluster-agent-admission-controller",
    "Namespace": "data",
    "UID": "istio://data/services/airflow-datadog-cluster-agent-admission-controller",
    "ExportTo": null,
    "LabelSelectors": {
      "app": "airflow-datadog-cluster-agent"
    },
    "ClusterExternalAddresses": null,
    "ClusterExternalPorts": null
  },
  "ports": [
    {
      "port": 443,
      "protocol": "UnsupportedProtocol"
    }
  ],
  "creationTime": "2022-11-23T05:46:05Z",
  "hostname": "airflow-datadog-cluster-agent-admission-controller.data.svc.cluster.local",
  "address": "10.100.94.238",
  "Mutex": {},
  "cluster-vips": {
    "cluster1": "10.100.94.238"
  },
  "Resolution": 0,
  "MeshExternal": false
},
```

The `cluster-vips` displays the virtual IP of the service in the corresponding
cluster. If you have the same service set up in two clusters and in the same
namespace, the you will see two records in `cluster-vips`, and istio will
round-robin requests to these two clusters, namely, this service is a
cross-cluster service.

There is a simpler way provided by `istioctl`.

```
istioctl proxy-config cluster <pod-name>.<namespace>
```

#### How does injection work

Istio installation will install a MutatingWebhookConfiguration
`istio-sidecar-injector` See blow sample output. As you can see that istiod has
an endpoint `/inject` does then injection work.

```
$ k describe mutatingwebhookconfiguration istio-sidecar-injector
...
    Service:
      Name:        istiod
      Namespace:   istio-system
      Path:        /inject
      Port:        443
...
```

Checkout
[code](https://github.com/istio/istio/blob/c36c5a5d36d6f77eed830fc13875b10459bfe5d6/pkg/kube/inject/webhook.go#L157)
for more details.

## Useful commands

- `istioctl version`: quickly get all running versions.
